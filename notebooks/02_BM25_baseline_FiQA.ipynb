{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c6ed281",
   "metadata": {},
   "source": [
    "# Baseline BM25 sobre FiQA – TFM_QA_RAG\n",
    "\n",
    "**Propósito:**  \n",
    "Implementar y evaluar el sistema de recuperación clásico BM25 sobre el subset limpio de FiQA, servirá de baseline para posteriores comparativas.\n",
    "\n",
    "**Archivos de entrada:**  \n",
    "- ../data/subset_corpus_clean.csv  \n",
    "- ../data/subset_queries_clean.csv  \n",
    "- ../data/subset_qrels_clean.csv\n",
    "\n",
    "---\n",
    "\n",
    "## Índice\n",
    "\n",
    "1. [Introducción y objetivos](#1-introducción-y-objetivos)\n",
    "2. [Carga de datos y preparación](#2-carga-de-datos-y-preparación)\n",
    "3. [Implementación de BM25](#3-implementación-de-bm25)\n",
    "4. [Evaluación de resultados](#4-evaluación-de-resultados)\n",
    "    - 4.1. Métricas agregadas (Precision@k, Recall@k, nDCG@k, MAP)\n",
    "    - 4.2. Análisis por query\n",
    "5. [Análisis cualitativo](#5-análisis-cualitativo)\n",
    "    - 5.1. Ejemplos de queries fáciles/difíciles para BM25\n",
    "    - 5.2. Limitaciones del enfoque clásico\n",
    "\n",
    "\n",
    "---\n",
    "## 1. Introducción y objetivos\n",
    "\n",
    "El objetivo de este notebook es implementar y evaluar el rendimiento del sistema clásico de recuperación de información BM25 sobre el subset limpio del dataset FiQA (Financial Question Answering), preparado previamente en el análisis exploratorio.\n",
    "\n",
    "El subset utilizado consta de:\n",
    "- **300 queries**\n",
    "- **3.000 documentos**\n",
    "- **778 pares relevantes (qrels)**\n",
    "\n",
    "Durante el análisis exploratorio se confirmó que:\n",
    "- Todas las queries tienen al menos un documento relevante, asegurando evaluabilidad real.\n",
    "- La mayoría de documentos extra funcionan como distractores reales, replicando un escenario de recuperación desafiante.\n",
    "- No existen duplicados ni textos vacíos; el campo `title` se eliminó por estar vacío en todos los casos.\n",
    "- El solapamiento léxico medio entre queries y documentos relevantes es de **0,40**.\n",
    "---\n",
    "## 2. Carga de datos y preparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2777c61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: (2998, 4)\n",
      "    _id                                               text metadata  \\\n",
      "0  1198  Yes, as long as you own the shares before the ...       {}   \n",
      "1  2003  \"While I haven't experienced being \"\"grad stud...       {}   \n",
      "\n",
      "   text_length  \n",
      "0           62  \n",
      "1           94   \n",
      "\n",
      "Queries: (300, 4)\n",
      "    _id                                               text metadata  \\\n",
      "0  3394  What is the easiest way to back-test index fun...       {}   \n",
      "1  5505  Can I deduct interest and fees on a loan for q...       {}   \n",
      "\n",
      "   text_length  \n",
      "0           11  \n",
      "1           13   \n",
      "\n",
      "Qrels: (778, 3)\n",
      "   query_id  doc_id  relevance\n",
      "0        15  325273          1\n",
      "1        18   88124          1\n",
      "\n",
      "Duplicados en corpus: 0\n",
      "Duplicados en queries: 0\n",
      "Duplicados en qrels: 0\n",
      "\n",
      "Valores nulos en corpus:\n",
      " _id            0\n",
      "text           0\n",
      "metadata       0\n",
      "text_length    0\n",
      "dtype: int64\n",
      "\n",
      "Valores nulos en queries:\n",
      " _id            0\n",
      "text           0\n",
      "metadata       0\n",
      "text_length    0\n",
      "dtype: int64\n",
      "\n",
      "Valores nulos en qrels:\n",
      " query_id     0\n",
      "doc_id       0\n",
      "relevance    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar archivos limpios\n",
    "corpus = pd.read_csv('../data/subset_corpus_clean.csv')\n",
    "queries = pd.read_csv('../data/subset_queries_clean.csv')\n",
    "qrels = pd.read_csv('../data/subset_qrels_clean.csv')\n",
    "\n",
    "#  estructura y tamaño de cada archivo\n",
    "print(\"Corpus:\", corpus.shape)\n",
    "print(corpus.head(2), \"\\n\")\n",
    "\n",
    "print(\"Queries:\", queries.shape)\n",
    "print(queries.head(2), \"\\n\")\n",
    "\n",
    "print(\"Qrels:\", qrels.shape)\n",
    "print(qrels.head(2))\n",
    "\n",
    "# Checks\n",
    "print(\"\\nDuplicados en corpus:\", corpus.duplicated().sum())\n",
    "print(\"Duplicados en queries:\", queries.duplicated().sum())\n",
    "print(\"Duplicados en qrels:\", qrels.duplicated().sum())\n",
    "\n",
    "print(\"\\nValores nulos en corpus:\\n\", corpus.isnull().sum())\n",
    "print(\"\\nValores nulos en queries:\\n\", queries.isnull().sum())\n",
    "print(\"\\nValores nulos en qrels:\\n\", qrels.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "304c8ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas en corpus: ['_id', 'text', 'metadata', 'text_length']\n",
      "Columnas en queries: ['_id', 'text', 'metadata', 'text_length']\n",
      "Columnas en qrels: ['query_id', 'doc_id', 'relevance']\n",
      "\n",
      "Info corpus:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2998 entries, 0 to 2997\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   _id          2998 non-null   int64 \n",
      " 1   text         2998 non-null   object\n",
      " 2   metadata     2998 non-null   object\n",
      " 3   text_length  2998 non-null   int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 93.8+ KB\n",
      "\n",
      "Info queries:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300 entries, 0 to 299\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   _id          300 non-null    int64 \n",
      " 1   text         300 non-null    object\n",
      " 2   metadata     300 non-null    object\n",
      " 3   text_length  300 non-null    int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 9.5+ KB\n",
      "\n",
      "Info qrels:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 778 entries, 0 to 777\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype\n",
      "---  ------     --------------  -----\n",
      " 0   query_id   778 non-null    int64\n",
      " 1   doc_id     778 non-null    int64\n",
      " 2   relevance  778 non-null    int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 18.4 KB\n"
     ]
    }
   ],
   "source": [
    "print(\"Columnas en corpus:\", corpus.columns.tolist())\n",
    "print(\"Columnas en queries:\", queries.columns.tolist())\n",
    "print(\"Columnas en qrels:\", qrels.columns.tolist())\n",
    "\n",
    "print(\"\\nInfo corpus:\")\n",
    "corpus.info()\n",
    "print(\"\\nInfo queries:\")\n",
    "queries.info()\n",
    "print(\"\\nInfo qrels:\")\n",
    "qrels.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b25f4",
   "metadata": {},
   "source": [
    "### Estrategias de tokenización evaluadas\n",
    "\n",
    "Se implementan y comparan varias variantes de tokenización:\n",
    "\n",
    "- **Básica:** minúsculas + split por espacio (sin eliminar puntuación ni stopwords).\n",
    "- **Normalización extra:** minúsculas + eliminación de puntuación/símbolos + split.\n",
    "- **Eliminación de stopwords:** minúsculas + eliminación de puntuación/símbolos + split + eliminación de stopwords (NLTK).\n",
    "- **Lematización:** minúsculas + eliminación de puntuación/símbolos + split + eliminación de stopwords + lematización (NLTK WordNetLemmatizer).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7d6b5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aalex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aalex\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Descargar recursos si no los tienes aún\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(text, mode='basic'):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Eliminar puntuación y símbolos si corresponde\n",
    "    if mode in ['nopunct', 'stopwords', 'lemmatize']:\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Split por espacios\n",
    "    tokens = text.split()\n",
    "    if mode == 'basic':\n",
    "        return tokens\n",
    "    if mode == 'stopwords':\n",
    "        tokens = [w for w in tokens if w not in stop_words]\n",
    "    if mode == 'lemmatize':\n",
    "        tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf23424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "def run_bm25_experiment(corpus_df, queries_df, qrels_df, token_mode, result_filename, top_k=10):\n",
    "    # Tokenizar corpus y queries\n",
    "    docs = corpus_df['text'].apply(lambda x: tokenize(x, mode=token_mode)).tolist()\n",
    "    queries = queries_df['text'].apply(lambda x: tokenize(x, mode=token_mode)).tolist()\n",
    "    doc_ids = corpus_df['_id'].tolist()\n",
    "    query_ids = queries_df['_id'].tolist()\n",
    "\n",
    "    bm25 = BM25Okapi(docs)\n",
    "    results = []\n",
    "    for q_idx, query_tokens in enumerate(queries):\n",
    "        scores = bm25.get_scores(query_tokens)\n",
    "        top_doc_idxs = np.argsort(scores)[::-1][:top_k]\n",
    "        for rank, idx in enumerate(top_doc_idxs):\n",
    "            doc_id = doc_ids[idx]\n",
    "            score = scores[idx]\n",
    "            is_rel = int(((qrels_df['query_id'] == query_ids[q_idx]) & (qrels_df['doc_id'] == doc_id)).any())\n",
    "            results.append({\n",
    "                'query_id': query_ids[q_idx],\n",
    "                'doc_id': doc_id,\n",
    "                'score': score,\n",
    "                'rank': rank+1,\n",
    "                'is_relevant': is_rel\n",
    "            })\n",
    "    # Guardar resultados\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f'../results/results_bm25_{token_mode}.csv', index=False)\n",
    "    print(f\"Resultados guardados en ../results/results_bm25_{token_mode}.csv\")\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2941b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, mode='basic'):\n",
    "    text = text.lower()\n",
    "    if mode in ['nopunct', 'stopwords', 'lemmatize']:\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    if mode == 'basic':\n",
    "        return tokens\n",
    "    if mode == 'nopunct':\n",
    "        return tokens\n",
    "    if mode == 'stopwords':\n",
    "        tokens = [w for w in tokens if w not in stop_words]\n",
    "    if mode == 'lemmatize':\n",
    "        tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec588ac",
   "metadata": {},
   "source": [
    "----\n",
    "## 3. Implementación de BM25\n",
    "Cada archivo contiene, por cada query, los documentos recuperados, su score, el ranking, y la etiqueta de relevancia (`is_relevant`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "535e434a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ejecutando BM25 con tokenización: basic ---\n",
      "Resultados guardados en ../results/results_bm25_basic.csv\n",
      "\n",
      "--- Ejecutando BM25 con tokenización: nopunct ---\n",
      "Resultados guardados en ../results/results_bm25_nopunct.csv\n",
      "\n",
      "--- Ejecutando BM25 con tokenización: stopwords ---\n",
      "Resultados guardados en ../results/results_bm25_stopwords.csv\n",
      "\n",
      "--- Ejecutando BM25 con tokenización: lemmatize ---\n",
      "Resultados guardados en ../results/results_bm25_lemmatize.csv\n"
     ]
    }
   ],
   "source": [
    "for mode in ['basic', 'nopunct', 'stopwords', 'lemmatize']:\n",
    "    print(f\"\\n--- Ejecutando BM25 con tokenización: {mode} ---\")\n",
    "    run_bm25_experiment(corpus, queries, qrels, token_mode=mode, result_filename=f'results_bm25_{mode}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4ad801",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Evaluación de resultados\n",
    "\n",
    "En esta sección se evalúan los resultados obtenidos por BM25 bajo las diferentes estrategias de tokenización:\n",
    "\n",
    "- **Precision@k**: Proporción de documentos relevantes entre los k primeros recuperados para cada query.\n",
    "- **Recall@k**: Proporción de los documentos relevantes totales que aparecen entre los k primeros recuperados.\n",
    "- **nDCG@k** (Normalized Discounted Cumulative Gain): Métrica que valora no solo la relevancia, sino la posición en el ranking.\n",
    "- **MAP** (Mean Average Precision): Promedio de la precisión obtenida en cada punto relevante a lo largo de todas las queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "582ae9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando BM25 (basic)...\n",
      "Evaluando BM25 (nopunct)...\n",
      "Evaluando BM25 (stopwords)...\n",
      "Evaluando BM25 (lemmatize)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision@1</th>\n",
       "      <th>Recall@1</th>\n",
       "      <th>nDCG@1</th>\n",
       "      <th>Precision@3</th>\n",
       "      <th>Recall@3</th>\n",
       "      <th>nDCG@3</th>\n",
       "      <th>Precision@5</th>\n",
       "      <th>Recall@5</th>\n",
       "      <th>nDCG@5</th>\n",
       "      <th>Precision@10</th>\n",
       "      <th>Recall@10</th>\n",
       "      <th>nDCG@10</th>\n",
       "      <th>MAP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variante</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>basic</th>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.163998</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.204444</td>\n",
       "      <td>0.275367</td>\n",
       "      <td>0.308292</td>\n",
       "      <td>0.147333</td>\n",
       "      <td>0.314999</td>\n",
       "      <td>0.311860</td>\n",
       "      <td>0.093000</td>\n",
       "      <td>0.384978</td>\n",
       "      <td>0.333482</td>\n",
       "      <td>0.261893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nopunct</th>\n",
       "      <td>0.423333</td>\n",
       "      <td>0.213543</td>\n",
       "      <td>0.423333</td>\n",
       "      <td>0.267778</td>\n",
       "      <td>0.362251</td>\n",
       "      <td>0.400553</td>\n",
       "      <td>0.197333</td>\n",
       "      <td>0.422902</td>\n",
       "      <td>0.412140</td>\n",
       "      <td>0.125333</td>\n",
       "      <td>0.521240</td>\n",
       "      <td>0.442866</td>\n",
       "      <td>0.357781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stopwords</th>\n",
       "      <td>0.446667</td>\n",
       "      <td>0.230364</td>\n",
       "      <td>0.446667</td>\n",
       "      <td>0.284444</td>\n",
       "      <td>0.389593</td>\n",
       "      <td>0.423066</td>\n",
       "      <td>0.209333</td>\n",
       "      <td>0.458282</td>\n",
       "      <td>0.438351</td>\n",
       "      <td>0.127333</td>\n",
       "      <td>0.536131</td>\n",
       "      <td>0.461636</td>\n",
       "      <td>0.378523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmatize</th>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.236123</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.412816</td>\n",
       "      <td>0.440240</td>\n",
       "      <td>0.214000</td>\n",
       "      <td>0.483444</td>\n",
       "      <td>0.456962</td>\n",
       "      <td>0.130333</td>\n",
       "      <td>0.552623</td>\n",
       "      <td>0.478584</td>\n",
       "      <td>0.396863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Precision@1  Recall@1    nDCG@1  Precision@3  Recall@3    nDCG@3  \\\n",
       "Variante                                                                      \n",
       "basic         0.340000  0.163998  0.340000     0.204444  0.275367  0.308292   \n",
       "nopunct       0.423333  0.213543  0.423333     0.267778  0.362251  0.400553   \n",
       "stopwords     0.446667  0.230364  0.446667     0.284444  0.389593  0.423066   \n",
       "lemmatize     0.450000  0.236123  0.450000     0.288889  0.412816  0.440240   \n",
       "\n",
       "           Precision@5  Recall@5    nDCG@5  Precision@10  Recall@10   nDCG@10  \\\n",
       "Variante                                                                        \n",
       "basic         0.147333  0.314999  0.311860      0.093000   0.384978  0.333482   \n",
       "nopunct       0.197333  0.422902  0.412140      0.125333   0.521240  0.442866   \n",
       "stopwords     0.209333  0.458282  0.438351      0.127333   0.536131  0.461636   \n",
       "lemmatize     0.214000  0.483444  0.456962      0.130333   0.552623  0.478584   \n",
       "\n",
       "                MAP  \n",
       "Variante             \n",
       "basic      0.261893  \n",
       "nopunct    0.357781  \n",
       "stopwords  0.378523  \n",
       "lemmatize  0.396863  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def evaluate_run(results_path, qrels, ks=[1,3,5,10]):\n",
    "    \"\"\"\n",
    "    Evalúa los resultados de un archivo (csv) tipo BM25/SBERT con las métricas estándar.\n",
    "    - results_path: path al csv con columnas [query_id, doc_id, score, rank, is_relevant]\n",
    "    - qrels: DataFrame con columnas [query_id, doc_id, relevance]\n",
    "    \"\"\"\n",
    "    results = pd.read_csv(results_path)\n",
    "    qrels_lookup = defaultdict(set)\n",
    "    for _, row in qrels.iterrows():\n",
    "        if row['relevance'] > 0:\n",
    "            qrels_lookup[row['query_id']].add(row['doc_id'])\n",
    "    \n",
    "    metrics = {k: {'precision': [], 'recall': [], 'ndcg': []} for k in ks}\n",
    "    average_precisions = []\n",
    "\n",
    "    for qid, group in results.groupby('query_id'):\n",
    "        retrieved = group.sort_values('rank')['doc_id'].tolist()\n",
    "        relevant = qrels_lookup[qid]\n",
    "        binary_relevance = [1 if doc in relevant else 0 for doc in retrieved]\n",
    "\n",
    "        for k in ks:\n",
    "            rel_at_k = binary_relevance[:k]\n",
    "            precision = np.mean(rel_at_k) if rel_at_k else 0.0\n",
    "            recall = (sum(rel_at_k) / len(relevant)) if relevant else 0.0\n",
    "\n",
    "            # nDCG@k\n",
    "            dcg = 0.0\n",
    "            for i, rel in enumerate(rel_at_k):\n",
    "                dcg += rel / np.log2(i + 2)\n",
    "            ideal_rel = [1]*min(len(relevant), k)\n",
    "            idcg = 0.0\n",
    "            for i, rel in enumerate(ideal_rel):\n",
    "                idcg += rel / np.log2(i + 2)\n",
    "            ndcg = dcg/idcg if idcg > 0 else 0.0\n",
    "\n",
    "            metrics[k]['precision'].append(precision)\n",
    "            metrics[k]['recall'].append(recall)\n",
    "            metrics[k]['ndcg'].append(ndcg)\n",
    "        \n",
    "        # AP (Average Precision)\n",
    "        n_rels = 0\n",
    "        ap = 0.0\n",
    "        for i, rel in enumerate(binary_relevance, 1):\n",
    "            if rel:\n",
    "                n_rels += 1\n",
    "                ap += n_rels / i\n",
    "        ap = ap / len(relevant) if relevant else 0.0\n",
    "        average_precisions.append(ap)\n",
    "\n",
    "    results_summary = {}\n",
    "    for k in ks:\n",
    "        results_summary[f'Precision@{k}'] = np.mean(metrics[k]['precision'])\n",
    "        results_summary[f'Recall@{k}'] = np.mean(metrics[k]['recall'])\n",
    "        results_summary[f'nDCG@{k}'] = np.mean(metrics[k]['ndcg'])\n",
    "    results_summary['MAP'] = np.mean(average_precisions)\n",
    "    return results_summary\n",
    "\n",
    "# ---- Evaluar todas las variantes BM25 ----\n",
    "qrels_clean = pd.read_csv('../data/subset_qrels_clean.csv')\n",
    "bm25_variants = ['basic', 'nopunct', 'stopwords', 'lemmatize']\n",
    "summary_table = []\n",
    "\n",
    "for mode in bm25_variants:\n",
    "    print(f\"Evaluando BM25 ({mode})...\")\n",
    "    summary = evaluate_run(f'../results/results_bm25_{mode}.csv', qrels_clean, ks=[1,3,5,10])\n",
    "    summary['Variante'] = mode\n",
    "    summary_table.append(summary)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_table).set_index('Variante')\n",
    "display(summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a398f26",
   "metadata": {},
   "source": [
    "### Resultados de la variante \"lemmatize\"\n",
    "\n",
    "Para la variante lematizada, los resultados por distintos valores de k son los siguientes:\n",
    "\n",
    "- **Precision@1:** 0.450\n",
    "- **Precision@3:** 0.289\n",
    "- **Precision@10:** 0.130\n",
    "\n",
    "El valor medio de documentos relevantes por query en el subset es **2.59**. Esto significa que, aunque solo hay un promedio de 2 o 3 documentos relevantes por pregunta, el sistema logra encontrar uno relevante en la primera posición en el 45% de los casos, y aumenta la probabilidad a medida que se amplía k, aunque la precisión baja como es esperable.\n",
    "\n",
    "- **Recall@1:** 0.236\n",
    "- **Recall@3:** 0.413\n",
    "- **Recall@10:** 0.553\n",
    "\n",
    "El recall, logicamente, sube con k, indicando que si se consideran más resultados (top-10), el sistema es capaz de recuperar una mayor parte de los documentos relevantes para cada query. Buena precisión en el top-1 y mejora de recall al ampliar el número de documentos recuperados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eef2df",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Análisis cualitativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19f6e426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EJEMPLOS DE QUERIES FÁCILES (relevante en posición 1):\n",
      "\n",
      "Query: How to account for money earned and spent prior to establishing business bank accounts?\n",
      "Doc recuperado (top-1): Funds earned and spent before opening a dedicated business account should be classified according to their origination. For example, if your business received income, where did that money go?  If you ...\n",
      "\n",
      "Query: financial institution wants share member break down for single member LLC\n",
      "Doc recuperado (top-1): \"What exactly would the financial institution need to see to make them   comfortable with these regulations The LLC Operating Agreement. The OA should specify the member's allocation of equity, assets...\n",
      "\n",
      "Query: Where to request ACH Direct DEBIT of funds from MY OWN personal bank account?\n",
      "Doc recuperado (top-1): Call Wells Fargo or go to a branch.  Tell them what you're trying to accomplish, not the vehicle you think you should use to get there.  Don't tell them you want to ACH DEBIT from YOUR ACCOUNT of YOUR...\n",
      "\n",
      "\n",
      "EJEMPLOS DE QUERIES DIFÍCILES (ningún relevante en top-10):\n",
      "\n",
      "Query: Applying for and receiving business credit\n",
      "Doc recuperado (top-1): Generally, banks will report your loan to at least one (if not all three) credit bureaus - although that is not required by law. The interest you're paying, in addition to your insurance isn't justifi...\n",
      "(Ningún documento relevante recuperado en el top-10)\n",
      "\n",
      "Query: What are the ins/outs of writing equipment purchases off as business expenses in a home based business?\n",
      "Doc recuperado (top-1): \"I'm not sure what you mean by \"\"writing off your time,\"\" but to answer your questions: Remember that, essentially, you are a salaried employee of a corporation.  So if you are spending time at your j...\n",
      "(Ningún documento relevante recuperado en el top-10)\n",
      "\n",
      "Query: Intentions of Deductible Amount for Small Business\n",
      "Doc recuperado (top-1): \"&gt;a startup has every intention of doing right.  Most self-identified \"\"startups\"\" I have known (as distinguished from actual profit-seeking \"\"small businesses\"\") have, as their primary intent, the...\n",
      "(Ningún documento relevante recuperado en el top-10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar resultados de BM25 lematizado y qrels\n",
    "results = pd.read_csv('../results/results_bm25_lemmatize.csv')\n",
    "qrels = pd.read_csv('../data/subset_qrels_clean.csv')\n",
    "queries = pd.read_csv('../data/subset_queries_clean.csv')\n",
    "corpus = pd.read_csv('../data/subset_corpus_clean.csv')\n",
    "\n",
    "# Asegúrate de que los tipos de ID son consistentes\n",
    "results['query_id'] = results['query_id'].astype(int)\n",
    "results['doc_id'] = results['doc_id'].astype(int)\n",
    "qrels['query_id'] = qrels['query_id'].astype(int)\n",
    "qrels['doc_id'] = qrels['doc_id'].astype(int)\n",
    "\n",
    "# Para cada query: ¿el relevante está en rank 1? (fácil) ¿ningún relevante en top-10? (difícil)\n",
    "def get_easy_hard_queries(results, qrels, top_k=10):\n",
    "    easy = []\n",
    "    hard = []\n",
    "    grouped = results.groupby('query_id')\n",
    "    for qid, group in grouped:\n",
    "        # docs recuperados y su relevancia (ordenados por rank)\n",
    "        topk = group.sort_values('rank').head(top_k)\n",
    "        relevant_docs = set(qrels[qrels['query_id'] == qid]['doc_id'])\n",
    "        # ¿Algún relevante en rank 1?\n",
    "        if topk.iloc[0]['is_relevant'] == 1:\n",
    "            easy.append(qid)\n",
    "        # ¿Ningún relevante en top-10?\n",
    "        if not any([doc in relevant_docs for doc in topk['doc_id']]):\n",
    "            hard.append(qid)\n",
    "    return easy, hard\n",
    "\n",
    "easy_qids, hard_qids = get_easy_hard_queries(results, qrels, top_k=10)\n",
    "\n",
    "# Mostrar ejemplos de queries fáciles\n",
    "print(\"\\nEJEMPLOS DE QUERIES FÁCILES (relevante en posición 1):\\n\")\n",
    "for qid in easy_qids[:3]:  # muestra los 3 primeros como ejemplo\n",
    "    q_text = queries.loc[queries['_id'] == qid, 'text'].values[0]\n",
    "    print(f\"Query: {q_text}\")\n",
    "    top1_doc_id = results[(results['query_id'] == qid) & (results['rank'] == 1)]['doc_id'].values[0]\n",
    "    doc_text = corpus.loc[corpus['_id'] == top1_doc_id, 'text'].values[0]\n",
    "    print(f\"Doc recuperado (top-1): {doc_text[:200]}...\\n\")\n",
    "\n",
    "# Mostrar ejemplos de queries difíciles\n",
    "print(\"\\nEJEMPLOS DE QUERIES DIFÍCILES (ningún relevante en top-10):\\n\")\n",
    "for qid in hard_qids[:3]:  # muestra los 3 primeros como ejemplo\n",
    "    q_text = queries.loc[queries['_id'] == qid, 'text'].values[0]\n",
    "    print(f\"Query: {q_text}\")\n",
    "    # Mostrar top-1 recuperado\n",
    "    top1_doc_id = results[(results['query_id'] == qid) & (results['rank'] == 1)]['doc_id'].values[0]\n",
    "    doc_text = corpus.loc[corpus['_id'] == top1_doc_id, 'text'].values[0]\n",
    "    print(f\"Doc recuperado (top-1): {doc_text[:200]}...\")\n",
    "    print(f\"(Ningún documento relevante recuperado en el top-10)\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa5aa2",
   "metadata": {},
   "source": [
    "#### Análisis cualitativo\n",
    "\n",
    "BM25 con lematización acierta cuando la query y el documento comparten palabras, si no hay coincidencia clara de términos, el sistema falla y no recupera los relevantes en el top-10.\n",
    "\n",
    "Este comportamiento es esperable en BM25, sobre todo cuando las queries y los documentos son cortos.  \n",
    "Sirve como baseline, pero tiene limitaciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca853005",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dc41c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando BM25 (basic)...\n",
      "Guardado en ../models/bm25_basic.pkl\n",
      "Guardando BM25 (nopunct)...\n",
      "Guardado en ../models/bm25_nopunct.pkl\n",
      "Guardando BM25 (stopwords)...\n",
      "Guardado en ../models/bm25_stopwords.pkl\n",
      "Guardando BM25 (lemmatize)...\n",
      "Guardado en ../models/bm25_lemmatize.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "bm25_variants = ['basic', 'nopunct', 'stopwords', 'lemmatize']\n",
    "\n",
    "for token_mode in bm25_variants:\n",
    "    print(f\"Guardando BM25 ({token_mode})...\")\n",
    "    docs_tokenized = corpus['text'].apply(lambda x: tokenize(x, mode=token_mode)).tolist()\n",
    "    bm25 = BM25Okapi(docs_tokenized)\n",
    "    with open(f'../models/bm25_{token_mode}.pkl', 'wb') as f:\n",
    "        pickle.dump(bm25, f)\n",
    "    print(f\"Guardado en ../models/bm25_{token_mode}.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77190715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
